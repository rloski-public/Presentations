{
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "# Demonstration\r\n",
        "This is code to demonstrate how to work with the **Azure Synapse Studio** notebooks.\r\n",
        "You can get more help at [Create, develop, and maintain Synapse notebooks in Azure Synapse Analytics](https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/apache-spark-development-using-notebooks).\r\n",
        "\r\n",
        "We will examine the following:\r\n",
        "\r\n",
        "1. Markdown cells [Markdown for Jupyter notebooks cheatsheet](https://www.ibm.com/docs/en/watson-studio-local/1.2.3?topic=notebooks-markdown-jupyter-cheatsheet)\r\n",
        "1. Working with the file magic commands\r\n",
        "1. Using the mssparkutil class\r\n",
        "1. Access file as dataset\r\n",
        "1. Display options\r\n",
        "    1. Show column information\r\n",
        "    1. Show sample data\r\n",
        "    1. Display in charts\r\n",
        "1. Using multiple languages\r\n",
        "1. Using SQL\r\n",
        "\r\n",
        "We won't go deep into how to actually develop.  My goal is to help you understand some of the mechanics of working with the notebooks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "print(\"Hello world!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## File magic commands\r\n",
        "The first thing I like to do is to get my bearings in the file system.\r\n",
        "There are enough places that I am going to mess up.  I need to make sure that the file \r\n",
        "is where I think that it is.\r\n",
        "\r\n",
        "These are some of the commands that **_might_** be available in Azure Synapse Studio.  https://ipython.readthedocs.io/en/stable/interactive/magics.html.\r\n",
        "\r\n",
        "Try %lsmagic to get a full list of the commands."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### lsmagic\r\n",
        "This provides a list of the magic commands with a link to the ipython document mentioned above."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%lsmagic fs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### fs magic to get the filesystem information\r\n",
        "This is where I start to learn about what is on the drive.\r\n",
        "\r\n",
        "The only documentation that I have found for the %fs magic is the mssparkutils documentation for the fs class.  https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python#file-system-utilities\r\n",
        "\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs ls \"abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/\"\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "What is in a file?\r\n",
        "\r\n",
        "`%fs head`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs head abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "%fs head abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv 1100"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## mssparkutil\r\n",
        "\r\n",
        "mssparkutil is a utility class that you can use in several scenarios.  You can get directories of files or you can change the configuration of the your spark session.  If you are needing to do anything advanced with the Spark environment, I would see what is availabe in mssparkutil.\r\n",
        "\r\n",
        "https://docs.microsoft.com/en-us/azure/synapse-analytics/spark/microsoft-spark-utilities?pivots=programming-language-python\r\n",
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Prepare by importing the library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from notebookutils import mssparkutils"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "folder = \"abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/\"\r\n",
        "files = mssparkutils.fs.ls(folder)\r\n",
        "for file in files:\r\n",
        "    print(file.name, file.isDir, file.isFile, file.path, file.size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "for file in [f for f in files if f.name.endswith(\".csv\")]:\r\n",
        "    print(file.name, file.isDir, file.isFile, file.path, file.size)\r\n",
        "    print(mssparkutils.fs.head(file.path, 2000))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Access file as dataset\r\n",
        "Once I know where the file is, I want to query the file.  I can use different languages and tools to do that.  I am most comfortable using Python.\r\n",
        "\r\n",
        "We won't delve deep into working with Python.\r\n",
        "\r\n",
        "Where I start is on the data tab.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df = spark.read.load('abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        "##, header=True\r\n",
        ")\r\n",
        "display(df.limit(10))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's add headers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df = spark.read.load('abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "display(df.limit(100))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Let's view the schema information."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df = spark.read.load('abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ")\r\n",
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "The schema is entirely strings for all columns. \r\n",
        "\r\n",
        "Notice"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Get a more detailed schema"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "df = spark.read.load('abfss://loskisynapsefilesystem@loskisynapsedatalake.dfs.core.windows.net/census/PUMS/1-year/2017/Population/psam_pusa.csv', format='csv'\r\n",
        "## If header exists uncomment line below\r\n",
        ", header=True\r\n",
        ", inferSchema=True\r\n",
        ")\r\n",
        "display(df.limit(100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "df.printSchema()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Now we are going restrict the number of columns to make it easier to work with the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import functions as func\r\n",
        "\r\n",
        "dfHisp = df.filter((df.AGEP < 65) & (df.HISP != 1 )).groupBy(\"HICOV\").agg(func.sum(\"PWGTP\").alias(\"TotalPopulation\"))\r\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "collapsed": false
      },
      "source": [
        "display(dfHisp)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Use a different language\r\n",
        "You can open a dataframe using one language and manipulate it using a different language"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "First you need to create a temporary table "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "dfHisp.createOrReplaceTempView(\"hispanicInsurance\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Run SQL"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "sparksql"
        },
        "collapsed": false
      },
      "source": [
        "%%sql\r\n",
        "SELECT * FROM hispanicInsurance"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "Use C#"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "microsoft": {
          "language": "csharp"
        }
      },
      "source": [
        "%%csharp\r\n",
        "\r\n",
        "DataFrame df = spark.Sql(\"SELECT * FROM hispanicInsurance\");\r\n",
        "\r\n",
        "df.Show();"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "## Snippets\r\n",
        "Type the word `Snippet` in a code cell and hit control-space. It will list some options for code snippets.  I was able to get this to work in Chrome but not Edge."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "\r\n",
        "# Use numpy to generate a bunch of random data in a bell curve around 5.\r\n",
        "n = 5 + np.random.randn(1000)\r\n",
        "\r\n",
        "m = [m for m in range(len(n))]\r\n",
        "plt.bar(m, n)\r\n",
        "plt.title(\"Raw Data\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.hist(n, bins=20)\r\n",
        "plt.title(\"Histogram\")\r\n",
        "plt.show()\r\n",
        "\r\n",
        "plt.hist(n, cumulative=True, bins=20)\r\n",
        "plt.title(\"Cumulative Histogram\")\r\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "metadata": {
        "jupyter": {
          "source_hidden": false,
          "outputs_hidden": false
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "from pyspark.sql import SparkSession \r\n",
        "from pyspark.sql.types import * \r\n",
        "\r\n",
        "# Primary storage info \r\n",
        "account_name = 'Your primary storage account name' # fill in your primary account name \r\n",
        "container_name = 'Your container name' # fill in your container name \r\n",
        "relative_path = 'Your relative path' # fill in your relative folder path \r\n",
        "\r\n",
        "adls_path = 'abfss://%s@%s.dfs.core.windows.net/%s' % (container_name, account_name, relative_path) \r\n",
        "print('Primary storage account path: ' + adls_path) \r\n",
        "\r\n",
        "# Read a csv file \r\n",
        "csv_path = adls_path + 'Your file name ' \r\n",
        "df_csv = spark.read.csv(csv_path, header = 'true') \r\n",
        "\r\n",
        "# Read a parquet file \r\n",
        "parquet_path = adls_path + ' Your file name ' \r\n",
        "df_parquet = spark.read.parquet(parquet_path) \r\n",
        "\r\n",
        "# Read a json file \r\n",
        "json_path = adls_path + 'Your file name ' \r\n",
        "df_json = spark.read.json(json_path) "
      ]
    }
  ],
  "metadata": {
    "description": "This is a notebook for demonstrating how to work with Azure Synapse Analytics notebooks.",
    "save_output": true,
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  }
}